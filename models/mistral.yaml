name: mistral
backend: llama-cpp
mmap: true
parameters:
  model: openchat_3.5.Q4_K_M.gguf
  temperature: 0.5
  top_p: 0.5
  top_k: 40
context_size: 4096
f16: true

threads: 2
gpu_layers: 50
low_vram: false
cuda: true

template:
  chat_message: chatml
  chat: chatml-block
  completion: completion

stopwords:
  - <|end_of_turn|>
